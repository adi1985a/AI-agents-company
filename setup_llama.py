#!/usr/bin/env python3
"""
Skrypt do konfiguracji modelu Bielik 11B v2.3
"""

import os
import sys
import subprocess
import requests
from pathlib import Path

def install_llama_cpp():
    """Instaluje llama-cpp-python"""
    print("Instalowanie llama-cpp-python...")
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "llama-cpp-python"])
        print("âœ… llama-cpp-python zainstalowane pomyÅ›lnie")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ BÅ‚Ä…d podczas instalacji: {e}")
        return False

def check_ollama():
    """Sprawdza czy Ollama jest zainstalowane i dostÄ™pne"""
    try:
        result = subprocess.run(["ollama", "--version"], capture_output=True, text=True)
        if result.returncode == 0:
            print(f"âœ… Ollama jest zainstalowane: {result.stdout.strip()}")
            return True
        else:
            print("âŒ Ollama nie jest zainstalowane lub nie dziaÅ‚a")
            return False
    except FileNotFoundError:
        print("âŒ Ollama nie jest zainstalowane")
        print("Zainstaluj Ollama z: https://ollama.ai")
        return False

def check_bielik_model():
    """Sprawdza czy model Bielik jest dostÄ™pny w Ollama"""
    try:
        result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
        if result.returncode == 0:
            if "bielik" in result.stdout.lower():
                print("âœ… Model Bielik jest dostÄ™pny w Ollama")
                return True
            else:
                print("âŒ Model Bielik nie jest dostÄ™pny w Ollama")
                return False
        else:
            print("âŒ Nie udaÅ‚o siÄ™ sprawdziÄ‡ listy modeli Ollama")
            return False
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d podczas sprawdzania modeli: {e}")
        return False

def pull_bielik_model():
    """Pobiera model Bielik do Ollama"""
    print("Pobieranie modelu Bielik 11B v2.3...")
    print("To moÅ¼e potrwaÄ‡ kilka minut...")
    
    try:
        result = subprocess.run([
            "ollama", "pull", "SpeakLeash/bielik-11b-v2.3-instruct:Q4_K_M"
        ], capture_output=True, text=True)
        
        if result.returncode == 0:
            print("âœ… Model Bielik zostaÅ‚ pobrany pomyÅ›lnie")
            return True
        else:
            print(f"âŒ BÅ‚Ä…d podczas pobierania modelu: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d podczas pobierania modelu: {e}")
        return False

def test_model():
    """Testuje czy model Bielik dziaÅ‚a poprawnie"""
    try:
        from llama_cpp import Llama
        
        print("Testowanie modelu Bielik...")
        llm = Llama(
            model_path="SpeakLeash/bielik-11b-v2.3-instruct:Q4_K_M",
            n_ctx=2048,
            n_threads=4,
            verbose=False
        )
        
        response = llm("Napisz 'CzeÅ›Ä‡'", max_tokens=10)
        print("âœ… Model Bielik dziaÅ‚a poprawnie!")
        return True
        
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d podczas testowania modelu: {e}")
        return False

def main():
    """GÅ‚Ã³wna funkcja setup"""
    print("=== Setup Bielik 11B v2.3 ===\n")
    
    # 1. SprawdÅº Ollama
    if not check_ollama():
        print("\nğŸ“‹ Instrukcje instalacji Ollama:")
        print("1. WejdÅº na: https://ollama.ai")
        print("2. Pobierz i zainstaluj Ollama")
        print("3. Uruchom ponownie: python setup_llama.py")
        return
    
    # 2. SprawdÅº czy model jest dostÄ™pny
    if not check_bielik_model():
        print("\nPobieranie modelu Bielik...")
        if not pull_bielik_model():
            print("\nğŸ“‹ Instrukcje rÄ™cznego pobierania:")
            print("1. OtwÃ³rz terminal/command prompt")
            print("2. Uruchom: ollama pull SpeakLeash/bielik-11b-v2.3-instruct:Q4_K_M")
            print("3. Poczekaj na pobranie (moÅ¼e potrwaÄ‡ kilka minut)")
            print("4. Uruchom ponownie: python setup_llama.py")
            return
    
    # 3. Instaluj llama-cpp-python
    if not install_llama_cpp():
        return
    
    # 4. Testuj model
    if test_model():
        print("\nğŸ‰ Setup zakoÅ„czony pomyÅ›lnie!")
        print("Model: SpeakLeash/bielik-11b-v2.3-instruct:Q4_K_M")
        print("MoÅ¼esz teraz uruchomiÄ‡ program z lokalnym modelem Bielik.")
    else:
        print("\nâŒ Setup nie powiÃ³dÅ‚ siÄ™.")

if __name__ == "__main__":
    main() 